{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Class Imputeddatasets\n",
    "class ImputedDatasets(Dataset):\n",
    "    \n",
    "    def __init__(self,HD_file='HD_file.rds',incplt_data_filename='df.rds',k=2):# from R to python and getting donors\n",
    "        self.HD=readRDS(HD_file)\n",
    "        self.incplt_data_filename=incplt_data_filename\n",
    "        self.k=k\n",
    "        self.incplt_data=self.incplt_data_rtopy()\n",
    "        self.donors=self.donors_gen()\n",
    "        self.whichna=self.whichna_mis()\n",
    "\n",
    "    \n",
    "    def incplt_data_rtopy(self):\n",
    "        incplt_data=readRDS(self.incplt_data_filename)\n",
    "        with localconverter(robjects.default_converter + pandas2ri.converter):# convert it to python DF\n",
    "                incplt_data = robjects.conversion.rpy2py(incplt_data)\n",
    "        return incplt_data\n",
    "\n",
    "    def donors_gen(self):\n",
    "        donors=list(range(len(self.HD[2])))\n",
    "        for i in range(len(self.HD[2])):\n",
    "            #donors of mis_value number i\n",
    "            donors[i]=self.HD[2][i]\n",
    "        return donors\n",
    "\n",
    "    def whichna_mis(self):\n",
    "        d=[[],[],[]]\n",
    "        for j in range(len(self.incplt_data.columns)):\n",
    "            for i in range(len(self.incplt_data)):\n",
    "                if np.isnan(self.incplt_data.iloc[i,j]) or self.incplt_data.iloc[i,j]==-2147483648:\n",
    "                    d[0].append(i)\n",
    "                    d[1].append(j)\n",
    "        for k in range(len(self.donors)): # \n",
    "            d[2].append(list(self.donors[k]))\n",
    "        d=pd.DataFrame(d)\n",
    "        whichna=pd.DataFrame.transpose(d)# whichna row 1 represents the first miss value in our incplt dataset, located in row=(col 0 of whichna)\n",
    "                                        # column=(col1 of whichna) and with plausible donors=(col2 of whichna)\n",
    "        return whichna\n",
    "\n",
    "    def sampling_row(self,i):\n",
    "        row=self.incplt_data.iloc[i,]#get the specified row from incplt data\n",
    "        r=[]\n",
    "        for imputation in range(self.k):# to get K samples for this specified row\n",
    "            r_imp=[]\n",
    "            for j in range(len(self.incplt_data.columns)):\n",
    "                if np.isnan(row[j]) or row[j]==-2147483648:\n",
    "                    donors=self.whichna[(self.whichna.iloc[:,0] ==i) & (self.whichna.iloc[:,1] ==j)].iloc[0,2]# fetch the donors in whichna using ij then sample (\n",
    "                    imp=random.choices(donors)[0]\n",
    "                else:\n",
    "                    imp=row[j]\n",
    "                r_imp.append(imp)\n",
    "            a=t.tensor(r_imp,dtype=t.float32)#turn the list to tensor..its shape is 13\n",
    "            b=a.expand(1,a.shape[0])#to get shape [1,12]\n",
    "            r.append(b)\n",
    "        row_imp=t.stack(r)#3D (k,1,12)\n",
    "        return row_imp\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "         return len(self.incplt_data)\n",
    "\n",
    "    def __getitem__(self,idx): \n",
    "        return self.sampling_row(idx)\n",
    "\n",
    "# defining functions\n",
    "## Training functions\n",
    "\n",
    "###RO-training\n",
    "def max_train(epochs,train_loader_imp,model,criterion,optimizer,device):\n",
    "    \"\"\"  \n",
    "    \n",
    "    \"\"\"\n",
    "    #train_loss=[]\n",
    "    for epoch in range (epochs):\n",
    "        epoch_loss=0\n",
    "        for data in train_loader_imp:\n",
    "            truth=data[...,[-1]].to(device=device)\n",
    "            input_model=data[...,:-1].to(device=device)\n",
    "            output=model(input_model)\n",
    "            loss=criterion(output,truth)\n",
    "            loss=t.mean(t.max(loss,dim=0)[0]) #(t.max : to get the max mse of each row, [0]: values, [1]: indices) \n",
    "            #print(loss)\n",
    "            epoch_loss+=loss.item()\n",
    "            #train_loss.append(loss)\n",
    "            loss.backward() # calcul grad\n",
    "            optimizer.step() # update weights & bias\n",
    "            optimizer.zero_grad()\n",
    "            # print(optimizer.param_groups)\n",
    "        print(\" epoch = %4d  loss = %0.4f  epoch_loss = %0.4f  training_loss=%0.4f \" % \\\n",
    "                (epoch, loss, epoch_loss, epoch_loss/len(train_loader_imp)))# the total number of batches\n",
    "    print(\"Done \")\n",
    "\n",
    "### Classic training\n",
    "def train_model(model,criterion,optimizer,train_loader,val_loader,epochs,device):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_loss=[]\n",
    "    val_loss=[]\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss=0\n",
    "        for data in (train_loader):\n",
    "            (X, Y) =  (data[...,:-1].to(device=device),data[...,[-1]].to(device=device)) # input ,targets\n",
    "            oupt = model(X) #predictions\n",
    "            loss = t.mean(criterion(oupt, Y))  # avg loss in BATCH\n",
    "            epoch_loss+=loss.item() #\n",
    "            loss.backward()# compute gradients\n",
    "            optimizer.step() # update weights\n",
    "            optimizer.zero_grad()\n",
    "        train_loss.append(epoch_loss/len(train_loader)) \n",
    "        # Validation phase\n",
    "        valid_loss=0\n",
    "        #model.eval() \n",
    "        for data in val_loader:\n",
    "            (X,Y)=(data[...,:-1],data[...,[-1]])\n",
    "            with t.no_grad():\n",
    "                oupt=model(X)\n",
    "            loss=t.mean(criterion(oupt,Y))\n",
    "            valid_loss+=loss.item()\n",
    "        val_loss.append(valid_loss/len(val_loader))\n",
    "        if epoch % 100 == 0:\n",
    "            print(\" epoch = %4d  loss = %0.4f  epoch_loss = %0.4f  training_loss=%0.4f Val_loss=%0.4f\"  % \\\n",
    "                (epoch, loss, epoch_loss, epoch_loss/len(train_loader),valid_loss / len(val_loader)))# the total number of batches\n",
    "    print(\"Done \")\n",
    "    plt.plot(train_loss,'-o')\n",
    "    plt.plot(val_loss,'-o')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Train','Valid'])\n",
    "    plt.title('Train vs Valid Loss')\n",
    "    plt.show\n",
    "\n",
    "## define testing functions \n",
    "\n",
    "def testing_loss(model,test_loader,device,imp=False):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    #percentage=1-\n",
    "    test_loss=0\n",
    "    for data in test_loader:\n",
    "        (X,Y)=(data[...,:-1].to(device=device),data[...,[-1]].to(device=device))\n",
    "        with t.no_grad():\n",
    "            pred = model(X)\n",
    "        loss=criterion(pred,Y)\n",
    "        if imp:\n",
    "            loss=t.mean(loss,dim=0)\n",
    "        summed_loss= t.sum(loss)\n",
    "        test_loss+=summed_loss.item()\n",
    "    test_loss=test_loss/len(test_loader.dataset)\n",
    "    if imp:\n",
    "        test_loss=test_loss/0.4 #percentage\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "def testing_max_loss(model,test_loader,device):# remove imp=false\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #percentage= 1-\n",
    "    test_loss=0\n",
    "    for data in test_loader:\n",
    "    # ... is a numpy/torch notation for any dimension that is not selected explicitely\n",
    "    #it allows me to work if the dataset is the imputed dataset\n",
    "        (X,Y)=(data[...,:-1].to(device=device),data[...,[-1]].to(device=device))\n",
    "        with t.no_grad():\n",
    "            pred=model(X)\n",
    "        loss=criterion(pred,Y)\n",
    "        loss=t.max(loss,dim=0)[0]\n",
    "        #changed for sum rather than mean\n",
    "        summed_loss=t.sum(loss)\n",
    "        test_loss+=summed_loss.item()\n",
    "    test_loss=test_loss/len(test_loader.dataset)*0.4 # the percentage of the test_imputed data \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instanciate_train_test_batch_sampler(length_dataset,percentage,batch_size,shuffle_train=True,shuffle_test=False,drop_last=False):\n",
    "    \"\"\"\n",
    "    Function to instanciate a generator for training and a generator for testing\n",
    "    Parameters:\n",
    "        length_dataset: int length of the dataset to sample\n",
    "        percentage: percentage of the datset for the training set\n",
    "        batch_size: batch size for the training\n",
    "        shffle_train: bool whether or not the training set should be shuffled\n",
    "        shuffle_test: bool whether or not the testing set should be shuffled\n",
    "        drop_last: bool whether or not to drop uncomplete batch\n",
    "    \"\"\"\n",
    "    # indices = np.random.choice(range(1,length_dataset+1),size=length_dataset,replace=False)\n",
    "    indices = np.random.choice(length_dataset,size=length_dataset,replace=False)\n",
    "    indices_train = indices[:int(percentage*length_dataset)]\n",
    "    indices_test = indices[int(percentage*length_dataset):]\n",
    "    if shuffle_train:\n",
    "        train_sampler = t.utils.data.SubsetRandomSampler(indices_train)\n",
    "    else:\n",
    "        train_sampler = indices_train\n",
    "    if shuffle_test:\n",
    "        test_sampler = t.utils.data.SubsetRandomSampler(indices_test)\n",
    "    else:\n",
    "        test_sampler = indices_test\n",
    "    return t.utils.data.BatchSampler(train_sampler, batch_size, drop_last), t.utils.data.BatchSampler(test_sampler, batch_size, drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# DATA AFTER IMPUTATION\n",
    "## RO-Imputed data\n",
    "k_train=1000\n",
    "results = ImputedDatasets(HD_file='HD_data1_MAR.rds',incplt_data_filename='scaled_data1_MAR.rds',k=k_train) \n",
    "# random.seed(4)\n",
    "# k_test=100\n",
    "# results_test = ImputedDatasets(HD_file='HD_data1.rds',incplt_data_filename='scaled_data1.rds',k=k_test)\n",
    "##Mean-imputation Data\n",
    "mean_imp=incplt_data.fillna(incplt_data.mean())     \n",
    "mean_imp=t.tensor(mean_imp.values).to(t.float32)\n",
    "##KNN-Imputed Data\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "data1_KNN= imputer.fit_transform(incplt_data)\n",
    "knn_data1=pd.DataFrame(data1_KNN)\n",
    "KNN_imp=t.tensor(knn_data1.values).to(t.float32)\n",
    "##MF-Imputed Data\n",
    "imputer = MissForest()\n",
    "data1_MF= imputer.fit_transform(incplt_data)\n",
    "MF_data=pd.DataFrame(data1_MF)\n",
    "MF_imp=t.tensor(MF_data.values).to(t.float32)\n",
    "\n",
    "\n",
    "# TRAIN/TEST LOADERS\n",
    "trsize=np.around(len(data1T)*0.6)\n",
    "valsize=np.around(len(data1T)*0.2)\n",
    "testsize=len(data1T)-trsize-valsize \n",
    "##train/val/test loaders for complete data\n",
    "train,val,test_cplt=random_split(data1T,[int(trsize),int(valsize),int(testsize)])\n",
    "batch_size=64\n",
    "train_loader_cplt=DataLoader(train,batch_size,shuffle=False)\n",
    "val_loader_cplt=DataLoader(val,batch_size,shuffle=False)\n",
    "test_loader_cplt=DataLoader(test_cplt,batch_size,shuffle=False)\n",
    "\n",
    "##train/ test loaders for RO imputed data\n",
    "collate_fn = lambda x: t.cat(x,dim=1)\n",
    "train_batch_sampler,test_batch_sampler=instanciate_train_test_batch_sampler(len(results),0.6,64,shuffle_train=True,shuffle_test=False,drop_last=False)\n",
    "train_loader_imp = DataLoader(results, batch_sampler=train_batch_sampler,shuffle=False, collate_fn=collate_fn)\n",
    "random.seed(4)\n",
    "test_loader_imp = DataLoader(results, batch_sampler=test_batch_sampler, shuffle=False, collate_fn=collate_fn)\n",
    "##train/val/test loaders for mean imputation data\n",
    "train,val,test_mean=random_split(mean_imp,[int(trsize),int(valsize),int(testsize)])\n",
    "batch_size=64\n",
    "mean_train_loader=DataLoader(train,batch_size,shuffle=True)\n",
    "mean_val_loader=DataLoader(val,batch_size,shuffle=False)\n",
    "mean_test_loader=DataLoader(test_mean,batch_size,shuffle=False)\n",
    "\n",
    "##train/val/test loaders for KNN imputed data\n",
    "train,val,test_knn=random_split(KNN_imp,[int(trsize),int(valsize),int(testsize)])\n",
    "batch_size=64\n",
    "KNN_train_loader=DataLoader(train,batch_size,shuffle=True)\n",
    "KNN_val_loader=DataLoader(val,batch_size,shuffle=False)\n",
    "KNN_test_loader=DataLoader(test_knn,batch_size,shuffle=False)\n",
    "\n",
    "#train/val/test loaders for MF imputed data \n",
    "train,val,test_mf=random_split(MF_imp,[int(trsize),int(valsize),int(testsize)])\n",
    "batch_size=64\n",
    "MF_train_loader=DataLoader(train,batch_size,shuffle=False)\n",
    "MF_val_loader=DataLoader(val,batch_size,shuffle=False)\n",
    "MF_test_loader=DataLoader(test_mf,batch_size,shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wanna learn more ?\n",
    "Contact me :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20a9e06a1eee47c4abbed4ec8225ad91d78d9800d202b71b6b0a6e47016c6abd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
